{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031fc7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 20:09:02.933714: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-10 20:09:02.933737: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>406 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.007264</td>\n",
       "      <td>7.006044</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>2 ROOM, Improved</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>232000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>108 ANG MO KIO AVENUE 4</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.271389</td>\n",
       "      <td>7.983837</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>602 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.069743</td>\n",
       "      <td>9.090700</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>262000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>465 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>0.946890</td>\n",
       "      <td>7.519889</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.083333</td>\n",
       "      <td>68.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>601 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.092551</td>\n",
       "      <td>9.130489</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133407</th>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "      <td>877 YISHUN STREET 81</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.475885</td>\n",
       "      <td>12.738721</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>EXECUTIVE, Maisonette</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>145.0</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>810000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133408</th>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774113</td>\n",
       "      <td>13.229106</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>785000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133409</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774113</td>\n",
       "      <td>13.229106</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.916667</td>\n",
       "      <td>171.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>842000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133410</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>632 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.700595</td>\n",
       "      <td>13.222912</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>845000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133411</th>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>605 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.603845</td>\n",
       "      <td>13.592586</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>163.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>862000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133412 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month  year              full_address   nearest_stn  \\\n",
       "0           1  2017  406 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "1           1  2017   108 ANG MO KIO AVENUE 4    Ang Mo Kio   \n",
       "2           1  2017   602 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "3           1  2017  465 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "4           1  2017   601 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "...       ...   ...                       ...           ...   \n",
       "133407      6  2022      877 YISHUN STREET 81        Khatib   \n",
       "133408      1  2022      633 YISHUN STREET 61        Khatib   \n",
       "133409      2  2022      633 YISHUN STREET 61        Khatib   \n",
       "133410      2  2022      632 YISHUN STREET 61        Khatib   \n",
       "133411      5  2022      605 YISHUN STREET 61        Khatib   \n",
       "\n",
       "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "0                  1.007264       7.006044           0.016807   \n",
       "1                  1.271389       7.983837           0.016807   \n",
       "2                  1.069743       9.090700           0.016807   \n",
       "3                  0.946890       7.519889           0.016807   \n",
       "4                  1.092551       9.130489           0.016807   \n",
       "...                     ...            ...                ...   \n",
       "133407             0.475885      12.738721           0.016807   \n",
       "133408             0.774113      13.229106           0.016807   \n",
       "133409             0.774113      13.229106           0.016807   \n",
       "133410             0.700595      13.222912           0.016807   \n",
       "133411             0.603845      13.592586           0.016807   \n",
       "\n",
       "        eigenvector_centrality                     flat_model_type  \\\n",
       "0                     0.006243                    2 ROOM, Improved   \n",
       "1                     0.006243              3 ROOM, New Generation   \n",
       "2                     0.002459              3 ROOM, New Generation   \n",
       "3                     0.006243              3 ROOM, New Generation   \n",
       "4                     0.002459              3 ROOM, New Generation   \n",
       "...                        ...                                 ...   \n",
       "133407                0.000968               EXECUTIVE, Maisonette   \n",
       "133408                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133409                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133410                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133411                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "\n",
       "        remaining_lease_years  floor_area_sqm storey_range  resale_price  \n",
       "0                   61.333333            44.0     10 TO 12      232000.0  \n",
       "1                   60.583333            67.0     01 TO 03      250000.0  \n",
       "2                   62.416667            67.0     01 TO 03      262000.0  \n",
       "3                   62.083333            68.0     04 TO 06      265000.0  \n",
       "4                   62.416667            67.0     01 TO 03      265000.0  \n",
       "...                       ...             ...          ...           ...  \n",
       "133407              64.583333           145.0     07 TO 09      810000.0  \n",
       "133408              65.000000           164.0     04 TO 06      785000.0  \n",
       "133409              64.916667           171.0     04 TO 06      842000.0  \n",
       "133410              64.750000           164.0     10 TO 12      845000.0  \n",
       "133411              64.750000           163.0     04 TO 06      862000.0  \n",
       "\n",
       "[133412 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the seed here is sufficient. \n",
    "# If you don't plan to use these starter code, make sure you add this cell.\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Normalization, StringLookup, IntegerLookup, Embedding,Input, Reshape,Flatten\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/hdb_price_prediction.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c771ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions in this cell are adapted from https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "# It is the same link as the one mentioned in the question paper (Q1b)\n",
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"resale_price\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\") # NOTE: as mentioned in the question paper, this actually does one-hot encoding. You could replace 'binary' with 'one_hot' if you wish to.\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "425801ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def r2(y_true, y_pred): \n",
    "    '''\n",
    "    # Obtained from https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/\n",
    "    # TODO: you have to find out how to use it in your code\n",
    "    '''\n",
    "    SS_res = K.sum(K.square( y_true - y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68d8a8",
   "metadata": {},
   "source": [
    "## a) Further split the data from year 2020 and before (i.e. those not in test set) by using data from year 2020 as validation set and the rest as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3258652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# function to encode categorical features as numbers as the DNN expects numbers and not strings\n",
    "def preprocess_df(df):\n",
    "    category_features = [ 'month', \n",
    "                         'flat_model_type', \n",
    "                         'storey_range']\n",
    "    \n",
    "    numeric_features = ['dist_to_nearest_stn', \n",
    "                        'dist_to_dhoby', \n",
    "                        'degree_centrality',\n",
    "                        'eigenvector_centrality', \n",
    "                        'remaining_lease_years', \n",
    "                        'floor_area_sqm']\n",
    "\n",
    "    for feature in category_features:\n",
    "        encoder = LabelEncoder()\n",
    "        df[feature] = encoder.fit_transform(df[feature])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7985f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625b3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant data , drop 'year' & 'resale_price' later\n",
    "df_cleaned = df_processed.drop(['full_address','nearest_stn'],axis=1)\n",
    "\n",
    "# Split data to train data (<2020 && >2020) and test data (== 2020)\n",
    "train_dataframe = df_cleaned[df_cleaned.year<2020]\n",
    "val_dataframe = df_cleaned[df_cleaned.year == 2020]\n",
    "\n",
    "# drop the remaining year data\n",
    "train_dataframe = train_dataframe.drop(['year'],axis=1)\n",
    "val_dataframe = val_dataframe.drop(['year'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d937e",
   "metadata": {},
   "source": [
    "## b) For each categorical variable, replace the one-hot encoding with the layer tf.keras.layers.Embedding(). Set output_dim = floor(num_categories//divisor). â€˜num_categoriesâ€™ refers to the number of categories in the categorical variable. â€˜divisorâ€™ is a parameter which we will tune later (Hint: You will still need the lookup classes from Q1b. Read the documentation to find out what to change.)\n",
    "\n",
    "> The Embedding layer produces a 2D output (3D, including batch), which cannot be concatenated with the other features. Add a Flatten layer to resolve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf51b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 20:09:05.196037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-10 20:09:05.196532: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-10 20:09:05.196652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-10 20:09:05.196736: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-10 20:09:05.199134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-10 20:09:05.199226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-10 20:09:05.199309: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-10 20:09:05.199321: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-10 20:09:05.199768: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)\n",
    "\n",
    "train_ds = train_ds.batch(256)\n",
    "val_ds = val_ds.batch(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d0253",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a263f",
   "metadata": {},
   "source": [
    "### building the input layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c085fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build input layer\n",
    "# returns model layer, inputs\n",
    "\n",
    "def build_input_layer(divisor):\n",
    "    # building input layer\n",
    "    numerical_features = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality','eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "    categorical_features = [ 'month', 'flat_model_type', 'storey_range']\n",
    "    \n",
    "    # keeping track of each category's cardinality in a dict\n",
    "    num_categorical_dict = {}\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        num_categorical_dict[feature] = df_cleaned[feature].nunique()\n",
    "\n",
    "    # building an embedding layer for each category column\n",
    "    inputs = []\n",
    "    models= []\n",
    "    \n",
    "    for cat,categorical_size in num_categorical_dict.items():\n",
    "\n",
    "        output_dim = categorical_size//divisor\n",
    "        vocab_size = categorical_size\n",
    "\n",
    "        # input layer for categorical data only takes 1 row\n",
    "        inpt = keras.Input(shape=(1,),name=cat)\n",
    "\n",
    "        # embed layer for categorical data \n",
    "        embed = Embedding(vocab_size,\n",
    "                          output_dim,\n",
    "                          trainable=True,\n",
    "                          embeddings_initializer=tf.initializers.random_normal)(inpt)\n",
    "\n",
    "        # Flatten\n",
    "        embed_reshaped = Flatten()(embed)\n",
    "        models.append(embed_reshaped)\n",
    "        inputs.append(inpt)\n",
    "        \n",
    "    # building layers for Numerical features    \n",
    "    dist_to_nearest_stn = keras.Input(shape=(1,), name=\"dist_to_nearest_stn\")\n",
    "    dist_to_dhoby = keras.Input(shape=(1,), name=\"dist_to_dhoby\")\n",
    "    degree_centrality =keras.Input(shape=(1,), name=\"degree_centrality\")\n",
    "    eigenvector_centrality=keras.Input(shape=(1,), name=\"eigenvector_centrality\")\n",
    "    remaining_lease_years=keras.Input(shape=(1,), name=\"remaining_lease_years\")\n",
    "    floor_area_sqm =keras.Input(shape=(1,), name=\"floor_area_sqm\")\n",
    "\n",
    "    # append all numerical inputs\n",
    "    inputs.append(dist_to_nearest_stn)\n",
    "    inputs.append(dist_to_dhoby)\n",
    "    inputs.append(degree_centrality)\n",
    "    inputs.append(eigenvector_centrality)\n",
    "    inputs.append(remaining_lease_years)\n",
    "    inputs.append(floor_area_sqm)\n",
    "\n",
    "    # encode all numerical features\n",
    "    # Numerical features\n",
    "    dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn,'dist_to_nearest_stn',train_ds)\n",
    "    dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby,'dist_to_dhoby',train_ds)\n",
    "    degree_centrality_encoded = encode_numerical_feature(degree_centrality,'degree_centrality',train_ds)\n",
    "    eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality,'eigenvector_centrality',train_ds)\n",
    "    remaining_lease_years_encoded = encode_numerical_feature(remaining_lease_years,'remaining_lease_years',train_ds)\n",
    "    floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm,'floor_area_sqm',train_ds)\n",
    "\n",
    "    #return encoded inputs\n",
    "    models+=[dist_to_nearest_stn_encoded,\n",
    "             dist_to_dhoby_encoded,\n",
    "             degree_centrality_encoded,\n",
    "             eigenvector_centrality_encoded,\n",
    "             remaining_lease_years_encoded,\n",
    "             floor_area_sqm_encoded]\n",
    "    # merging embedded categorical layers and numerical layers to a single input layer\n",
    "    \n",
    "    all_features = layers.concatenate(models)\n",
    "    \n",
    "    \n",
    "    return all_features,inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff373af",
   "metadata": {},
   "source": [
    "\n",
    "## c) Via a callback, introduce early stopping (based on val_loss, with patience of 10 epochs) to the model.\n",
    "> Using this as a reference, use KerasTuner (with the RandomSearch algorithm) to tune the model on the validation set, according to the following ranges:\n",
    "* Number of neurons: min=4, max=32, step=4\n",
    "* Learning rate: min=1e-4, max=2e-1, sampling=â€™logâ€™\n",
    "* Divisor: min=1, max=2, step=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9009c",
   "metadata": {},
   "source": [
    "### Early stop callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fefda227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stop callback\n",
    "def early_stop_callback():\n",
    "  return tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462cfc2",
   "metadata": {},
   "source": [
    "### some other useful callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "876b191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a callback function to persist r2 values from each epoch\n",
    "\n",
    "class storeInformationCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.store =[]\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        self.store.append([epoch,logs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c28c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to save the entire model\n",
    "def saveModelsCallback(path,monitor,mode,save_freq):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path,\n",
    "        monitor = monitor,\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = False,\n",
    "        mode = mode,\n",
    "        save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5f32d",
   "metadata": {},
   "source": [
    "### Building the entire layer stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a588356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "        \n",
    "    # stack the layers\n",
    "    input_layer,inputs = build_input_layer(hp.Int('divisor',min_value=1,max_value=2,step=1))\n",
    "    \n",
    "    # building the rest of the model\n",
    "    hidden_layer = layers.Dense(units=hp.Int('units',min_value=4,max_value=32,step=4),\n",
    "                                activation='relu')(input_layer)\n",
    "    \n",
    "    output_layer = layers.Dense(1,activation='linear')(hidden_layer)\n",
    "    \n",
    "    new_model = keras.Model(inputs,output_layer)\n",
    "    \n",
    "    # compiling the model\n",
    "    new_model.compile(optimizer = tf.keras.optimizers\n",
    "                      .Adam(learning_rate = hp.Float('learning_rate',\n",
    "                                                     min_value = 1e-4,\n",
    "                                                     max_value = 2e-1,\n",
    "                                                     sampling = 'log')),\n",
    "                      metrics = [r2,'mean_squared_error'],loss='mean_squared_error')\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9387cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "tuner.search(train_ds,\n",
    "             validation_data=val_ds, \n",
    "             epochs=50,\n",
    "             callbacks=[early_stop_callback()],\n",
    "             verbose=0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dceec49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best divisor: 2\n",
      "best learning rate: 0.06805250998340012\n",
      "best units: 16\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "optimum_divisor = best_hyperparameters.get('divisor')\n",
    "optimum_learning_rate = best_hyperparameters.get('learning_rate')\n",
    "optimum_units = best_hyperparameters.get('units')\n",
    "\n",
    "print('best divisor: {}'.format(optimum_divisor))\n",
    "print('best learning rate: {}'.format(optimum_learning_rate))\n",
    "print('best units: {}'.format(optimum_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548bfd1",
   "metadata": {},
   "source": [
    "# d) Using the best model configuration, train a model on the non-test split (i.e. year 2020 and before) for 50 epochs. Generate a plot to show how the train and test root mean square errors (RMSE) changes across epochs. (Tip: You can skip the first few epochs if the plot gets dominated by them)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f120157f",
   "metadata": {},
   "source": [
    "Using callback function to write the best epoch the model was saved into the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e08554f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback to save the entire model\n",
    "def saveModelsCallbackCustom(path,monitor,mode,save_freq):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path+'{epoch:02d}',\n",
    "        monitor = monitor,\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = False,\n",
    "        mode = mode,\n",
    "        save_freq=save_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb362d",
   "metadata": {},
   "source": [
    "### building model using the optimum hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d34d108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 10241594368.00000, saving model to ./data/models/PartB_Q2/2d/best_model01\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model01/assets\n",
      "\n",
      "Epoch 2: val_loss improved from 10241594368.00000 to 8443603456.00000, saving model to ./data/models/PartB_Q2/2d/best_model02\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model02/assets\n",
      "\n",
      "Epoch 3: val_loss improved from 8443603456.00000 to 7806822400.00000, saving model to ./data/models/PartB_Q2/2d/best_model03\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model03/assets\n",
      "\n",
      "Epoch 4: val_loss improved from 7806822400.00000 to 7343005184.00000, saving model to ./data/models/PartB_Q2/2d/best_model04\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model04/assets\n",
      "\n",
      "Epoch 5: val_loss improved from 7343005184.00000 to 7023431168.00000, saving model to ./data/models/PartB_Q2/2d/best_model05\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model05/assets\n",
      "\n",
      "Epoch 6: val_loss improved from 7023431168.00000 to 6533711872.00000, saving model to ./data/models/PartB_Q2/2d/best_model06\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model06/assets\n",
      "\n",
      "Epoch 7: val_loss improved from 6533711872.00000 to 6099994624.00000, saving model to ./data/models/PartB_Q2/2d/best_model07\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model07/assets\n",
      "\n",
      "Epoch 8: val_loss improved from 6099994624.00000 to 5877494784.00000, saving model to ./data/models/PartB_Q2/2d/best_model08\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model08/assets\n",
      "\n",
      "Epoch 9: val_loss improved from 5877494784.00000 to 5410183680.00000, saving model to ./data/models/PartB_Q2/2d/best_model09\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model09/assets\n",
      "\n",
      "Epoch 10: val_loss improved from 5410183680.00000 to 5336745984.00000, saving model to ./data/models/PartB_Q2/2d/best_model10\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model10/assets\n",
      "\n",
      "Epoch 11: val_loss improved from 5336745984.00000 to 5030095360.00000, saving model to ./data/models/PartB_Q2/2d/best_model11\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model11/assets\n",
      "\n",
      "Epoch 12: val_loss improved from 5030095360.00000 to 5025747968.00000, saving model to ./data/models/PartB_Q2/2d/best_model12\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model12/assets\n",
      "\n",
      "Epoch 13: val_loss improved from 5025747968.00000 to 4679065600.00000, saving model to ./data/models/PartB_Q2/2d/best_model13\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model13/assets\n",
      "\n",
      "Epoch 14: val_loss improved from 4679065600.00000 to 4518825984.00000, saving model to ./data/models/PartB_Q2/2d/best_model14\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model14/assets\n",
      "\n",
      "Epoch 15: val_loss improved from 4518825984.00000 to 4433583616.00000, saving model to ./data/models/PartB_Q2/2d/best_model15\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model15/assets\n",
      "\n",
      "Epoch 16: val_loss improved from 4433583616.00000 to 4362022912.00000, saving model to ./data/models/PartB_Q2/2d/best_model16\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model16/assets\n",
      "\n",
      "Epoch 17: val_loss improved from 4362022912.00000 to 4305037824.00000, saving model to ./data/models/PartB_Q2/2d/best_model17\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model17/assets\n",
      "\n",
      "Epoch 18: val_loss did not improve from 4305037824.00000\n",
      "\n",
      "Epoch 19: val_loss improved from 4305037824.00000 to 4113648384.00000, saving model to ./data/models/PartB_Q2/2d/best_model19\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model19/assets\n",
      "\n",
      "Epoch 20: val_loss improved from 4113648384.00000 to 3895816960.00000, saving model to ./data/models/PartB_Q2/2d/best_model20\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model20/assets\n",
      "\n",
      "Epoch 21: val_loss improved from 3895816960.00000 to 3789078016.00000, saving model to ./data/models/PartB_Q2/2d/best_model21\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model21/assets\n",
      "\n",
      "Epoch 22: val_loss did not improve from 3789078016.00000\n",
      "\n",
      "Epoch 23: val_loss improved from 3789078016.00000 to 3745075712.00000, saving model to ./data/models/PartB_Q2/2d/best_model23\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model23/assets\n",
      "\n",
      "Epoch 24: val_loss improved from 3745075712.00000 to 3593894400.00000, saving model to ./data/models/PartB_Q2/2d/best_model24\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model24/assets\n",
      "\n",
      "Epoch 25: val_loss improved from 3593894400.00000 to 3504462336.00000, saving model to ./data/models/PartB_Q2/2d/best_model25\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model25/assets\n",
      "\n",
      "Epoch 26: val_loss did not improve from 3504462336.00000\n",
      "\n",
      "Epoch 27: val_loss improved from 3504462336.00000 to 3399276800.00000, saving model to ./data/models/PartB_Q2/2d/best_model27\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model27/assets\n",
      "\n",
      "Epoch 28: val_loss improved from 3399276800.00000 to 3348300032.00000, saving model to ./data/models/PartB_Q2/2d/best_model28\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model28/assets\n",
      "\n",
      "Epoch 29: val_loss did not improve from 3348300032.00000\n",
      "\n",
      "Epoch 30: val_loss did not improve from 3348300032.00000\n",
      "\n",
      "Epoch 31: val_loss did not improve from 3348300032.00000\n",
      "\n",
      "Epoch 32: val_loss improved from 3348300032.00000 to 3182273024.00000, saving model to ./data/models/PartB_Q2/2d/best_model32\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model32/assets\n",
      "\n",
      "Epoch 33: val_loss did not improve from 3182273024.00000\n",
      "\n",
      "Epoch 34: val_loss improved from 3182273024.00000 to 3153737728.00000, saving model to ./data/models/PartB_Q2/2d/best_model34\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model34/assets\n",
      "\n",
      "Epoch 35: val_loss did not improve from 3153737728.00000\n",
      "\n",
      "Epoch 36: val_loss improved from 3153737728.00000 to 3042815488.00000, saving model to ./data/models/PartB_Q2/2d/best_model36\n",
      "INFO:tensorflow:Assets written to: ./data/models/PartB_Q2/2d/best_model36/assets\n",
      "\n",
      "Epoch 37: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 38: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 39: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 40: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 41: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 42: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 43: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 44: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 45: val_loss did not improve from 3042815488.00000\n",
      "\n",
      "Epoch 46: val_loss did not improve from 3042815488.00000\n"
     ]
    }
   ],
   "source": [
    "input_layer,inputs = build_input_layer(optimum_divisor)\n",
    "\n",
    "# building the rest of the model\n",
    "hidden_layer = layers.Dense(units=optimum_units,\n",
    "                            activation='relu')(input_layer)\n",
    "\n",
    "output_layer = layers.Dense(1,activation='linear')(hidden_layer)\n",
    "\n",
    "optimum_model = keras.Model(inputs,output_layer)\n",
    "\n",
    "# compiling the model\n",
    "optimum_model.compile(optimizer = tf.keras.optimizers\n",
    "                  .Adam(learning_rate = optimum_learning_rate),\n",
    "                  metrics = [r2,tf.keras.metrics.RootMeanSquaredError()],loss='mean_squared_error')\n",
    "\n",
    "#callback for persisting logs\n",
    "storage = storeInformationCallback()\n",
    "\n",
    "# fit and save the optimum model\n",
    "history = optimum_model.fit(train_ds,\n",
    "                            epochs=50,\n",
    "                            validation_data=val_ds,\n",
    "                            verbose=0,\n",
    "                            callbacks=[\n",
    "                                storage,\n",
    "                                early_stop_callback(),\n",
    "                                saveModelsCallbackCustom(path='./data/models/PartB_Q2/2d/best_model',\n",
    "                                                   monitor='val_loss',\n",
    "                                                   mode='min',\n",
    "                                                   save_freq='epoch')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44ed9f",
   "metadata": {},
   "source": [
    "### plot to show how the train and test root mean square errors (RMSE) changes across epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29a67f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'RMSE')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAAsTAAALEwEAmpwYAABAnklEQVR4nO3dd3gVVfrA8e+bTgoJpAGJQAi9945SpLoCIqAIioiiLmBnhV131V1317XrT0VpCiJNREXpKHZ6770klCS0EEr6+f0xAwZI597ckLyf55kn9545M/POKHkzc86cI8YYlFJKqcJwc3UASimlbl6aRJRSShWaJhGllFKFpklEKaVUoWkSUUopVWgerg6gqIWEhJiqVau6OgyllLqprF+//qQxJvTa8lKXRKpWrcq6detcHYZSSt1URORwduX6OEsppVShaRJRSilVaJpElFJKFVqpaxNRSqmCSktLIzY2luTkZFeH4nQ+Pj5ERkbi6emZr/qaRJRSKg+xsbEEBARQtWpVRMTV4TiNMYZTp04RGxtLVFRUvrbRx1lKKZWH5ORkgoODS3QCARARgoODC3THpUlEKaXyoaQnkMsKep6aRPIhM9MwY/URFm497upQlFKqWNEkkg9ubsKstUd4e9kedP4VpVRRO3v2LB9++GGhtn3nnXe4ePGigyP6gyaRfBrSugp748+z+uBpV4eilCplinMS0d5Z+XRnw0r8e8FOpq86TOtqwa4ORylViowdO5b9+/fTuHFjunbtSlhYGHPmzCElJYW77rqLl19+mQsXLjBw4EBiY2PJyMjg73//O3FxcRw7doxOnToREhLCihUrHB6bU5OIiDwNPAwYYCswDKgIzAKCgfXA/caYVBHxBqYBzYBTwD3GmEP2fsYBw4EM4AljzBK7vAfwLuAOTDLGvOqscynj5U7/ZpFM/f0Q8UnJhAX4OOtQSqli7OVvt7Pj2DmH7rNupbK8eGe9HNe/+uqrbNu2jU2bNrF06VLmzp3LmjVrMMbQu3dvfv75ZxISEqhUqRILFiwAIDExkcDAQN566y1WrFhBSEiIQ2O+zGmPs0QkAngCaG6MqY/1i/5e4H/A28aY6sAZrOSA/fOMXf62XQ8RqWtvVw/oAXwoIu4i4g58APQE6gKD7LpOM7hVZdIzDXPWxjjzMEoplaOlS5eydOlSmjRpQtOmTdm1axd79+6lQYMGLFu2jOeff55ffvmFwMDAIonH2Y+zPIAyIpIG+ALHgc7Affb6qcBLwHigj/0ZYC7wvlh9zfoAs4wxKcBBEdkHtLTr7TPGHAAQkVl23R3OOplqof60rx7CjNVHeOy2aDzctUlJqdImtzuGomCMYdy4cTz66KPXrduwYQMLFy7khRdeoEuXLvzjH/9wejxO+y1ojDkKvAEcwUoeiViPr84aY9LtarFAhP05Aoixt0236wdnLb9mm5zKryMiI0RknYisS0hIuKHzGtK6CscSk/lhV/wN7UcppfIrICCApKQkALp3786UKVM4f/48AEePHiU+Pp5jx47h6+vLkCFDGDNmDBs2bLhuW2dw2p2IiJTDujOIAs4CX2A9jipyxpgJwASA5s2b31Af3dvrhFGhrA/TVx+hW70KDolPKaVyExwcTLt27ahfvz49e/bkvvvuo02bNgD4+/szffp09u3bx5gxY3Bzc8PT05Px48cDMGLECHr06EGlSpVuuob124GDxpgEABGZB7QDgkTEw77biASO2vWPArcAsSLiAQRiNbBfLr8s6zY5lTuNh7sbg1pW5u3lezh08gJVQ/ycfUillGLGjBlXfX/yySev+h4dHU337t2v22706NGMHj3aaXE586H+EaC1iPjabRtdsNorVgD97TpDgW/sz/Pt79jrfzDWm33zgXtFxFtEooAawBpgLVBDRKJExAur8X2+U84kMxO2fAG7FwFwb8tb8HATZqw54pTDKaXUzcKZbSKrsRrIN2B173XDeqT0PPCM3UAeDEy2N5kMBNvlzwBj7f1sB+ZgJaDFwEhjTIZ9JzMKWALsBObYdR1PBH57F5a/DMYQXtaH7vUqMGddDMlpGU45pFJK3Qyc2jvLGPMi8OI1xQf4o3dV1rrJwIAc9vNv4N/ZlC8EFt54pHkQgdaPwTcj4eBPUK0jg1tXZsHW43y35Tj9m0U6PQSllCqOtI9qftXvD74hsMpqrGpTLZjoUD+mr8p27nqllCoVNInkl6cPNH8I9iyBU/sREYa0rsKmmLNsO5ro6uiUUsolNIkURIvh4OYBayYA0K9pJGU83fVuRClVamkSKYiAClDvLtj4OSSfI7CMJ32bVOLrTUdJvJTm6uiUUiVUYUfx7dWrF2fPnnV8QFloEimo1o9BahJs+hyAwa2qkJyWyZfrY10cmFKqpMopiaSnp2dT+w8LFy4kKCjISVFZNIkUVEQziGwJqz+CzAzqRwTSpHIQ01cf1gmrlFJOkXUo+BYtWtChQwd69+5N3brWmLN9+/alWbNm1KtXjwkTJlzZrmrVqpw8eZJDhw5Rp04dHnnkEerVq0e3bt24dOmSQ2LT+UQKo/XjMHeY1cheuxdDWlXh2S82M3/zMfo0znb4LqVUSbFoLJzY6th9VmgAPXOeySLrUPA//vgjd9xxB9u2bSMqKgqAKVOmUL58eS5dukSLFi24++67CQ6+et6jvXv3MnPmTCZOnMjAgQP58ssvGTJkyA2HrncihVHnTigbAaut7r69G1eiaeUgxs3byt445w10ppRSAC1btrySQADee+89GjVqROvWrYmJiWHv3r3XbRMVFUXjxo0BaNasGYcOHXJILHonUhjuntDiYfj+ZYjbjmd4PT4c3Iw//d8vPDp9Pd+MbEeAj6ero1RKOUMudwxFxc/vjzH7fvzxR5YvX87KlSvx9fWlY8eOJCcnX7eNt7f3lc/u7u4Oe5yldyKF1exB8ChjtY0AFQJ9eP++phw+dZExX2zR9hGllMPkNpx7YmIi5cqVw9fXl127drFq1aoijU2TSGH5loeGA2HLHLhwCoDW1YIZ17M2i7ef4OOfD7g4QKVUSZF1KPgxY8Zcta5Hjx6kp6dTp04dxo4dS+vWrYs0NiltfzE3b97crFu3zjE7i98JH7aGzn+HW58DrFnHRs3cyKKtx/lseCvaVXfOvMZKqaKzc+dO6tSp4+owikx25ysi640xza+tq3ciNyKsDlTrCGsnQ4b1sqGI8NrdDYkO9Wf0zI0cPeuY545KKVUcaRK5Ua0eh6RjsOObK0V+3h58dH8zUtMz+fP09aSk63DxSqmSSZPIjarRDcpXu9LAfll0qD9vDGjE5thEXpq/w0XBKaUcpbQ8+i/oeWoSuVFubtDyUYhdCzFrr1rVo34FHu8Yzcw1R5izNsZFASqlbpSPjw+nTp0q8YnEGMOpU6fw8fHJ9zbasO4IKefh7XpQuQ3cN+uqVekZmdw/eQ3bjiWy9m+34+Pp7thjK6WcLi0tjdjY2GzfvyhpfHx8iIyMxNPz6nfdcmpY15cNHcHb3xoK5cf/woltUKH+lVUe7m78uVM0909eww+74unVoKILA1VKFYanp+dVb4irP+jjLEdpOQK8/OGXN69b1TY6hPCy3szbcNQFgSmllPNoEnEU3/LWUCjbv4KT+65a5e4m9GkcwY+74zl9IdVFASqllONpEnGkNiPBwxt+ffu6VX0bR5CeaViw5ZgLAlNKKefQJOJI/mHQdChsmQVnj1y1qm6lstSuEMC8jfpISylVcmgScbR2TwACv7133aq7mkSw8chZDp68UPRxKaWUE2gScbTASGh0L2yYBkknrlrVu3ElROBrvRtRSpUQTksiIlJLRDZlWc6JyFMi8pKIHM1S3ivLNuNEZJ+I7BaR7lnKe9hl+0RkbJbyKBFZbZfPFhEvZ51PgbR/GjLTYOX7VxVXDCxD2+hgvt50tMS/tKSUKh2clkSMMbuNMY2NMY2BZsBF4Ct79duX1xljFgKISF3gXqAe0AP4UETcRcQd+ADoCdQFBtl1Af5n76s6cAYY7qzzKZDgaKjXD9ZOgYunr1rVt3EEh09dZMORMy4KTimlHKeoHmd1AfYbYw7nUqcPMMsYk2KMOQjsA1rayz5jzAFjTCowC+gjIgJ0Buba208F+jrrBAqsw7OQduG6MbV6NqiIj6cbX+kjLaVUCVBUSeReYGaW76NEZIuITBGRcnZZBJB1gKlYuyyn8mDgrDEm/Zry4iG8LtT+k5VEks9dKfb39qBb3Qp8t+U4qemZLgxQKaVunNOTiN1O0Rv4wi4aD0QDjYHjwPWveDs+hhEisk5E1iUkJDj7cH/o8CwkJ8K6yVcV39UkgrMX01ixO77oYlFKKScoijuRnsAGY0wcgDEmzhiTYYzJBCZiPa4COArckmW7SLssp/JTQJCIeFxTfh1jzARjTHNjTPPQ0FAHnVY+RDSF6M7w+/uQevFKcYcaIYT4e2kvLaXUTa8oksggsjzKEpGsIxDeBWyzP88H7hURbxGJAmoAa4C1QA27J5YX1qOx+cbq3rQC6G9vPxT4huKmw3Nw8aTV5dfm4e7GnY0q8f3OeBIvprkwOKWUujFOTSIi4gd0BeZlKX5NRLaKyBagE/A0gDFmOzAH2AEsBkbadyzpwChgCbATmGPXBXgeeEZE9mG1kVz93Kg4qNrOGiL+t3euuhu5q0kEqRmZLNh63HWxKaXUDdL5RIrCod/g017Q+e9w63OANfnL7W/9RLCfN3Mea1O08SilVAHlNJ+IvrFeFKq2g1q94Nd34LzVsC8i3NUkgjWHThNz+mLu2yulVDGlSaSo3P4ypF2En/53pahPY6tHsjawK6VuVppEikpoTWg2FNZ/cmW+kVvK+9Iyqjxf6TAoSqmblCaRotRxHHj4wPIXrxTd1SSCAwkX2HDkrOviUkqpQtIkUpT8w6Ddk7DrOzi8EoBeDSoS4O3BoImreH7uFvbEJbk4SKWUyj9NIkWtzUjwrwDL/g7GEFjGk/mj2zOweSTfbD5Kt7d/5oEpa/h5T4I+4lJKFXvaxdcVNkyD+aNhwKdQ764rxWcupDJjzRE+/f0QCUkp1Az35+H21ejduBI+nu6ui1cpVerl1MVXk4grZGbAR+0h7RKMXAMeV0+DkpKewXebjzPxlwPsOpFEpUAf5o9uT4i/t4sCVkqVdvqeSHHi5g5d/wVnDl43OCOAt4c7dzeLZNGTHfhkWAviklL4YMU+FwSqlFK50yTiKtW7QLWO1nsjl85mW0VE6FQrjP5NI/l81RFiz+hLiUqp4kWTiKuIWHcjl87Cr2/lWvXJ22uAwDvL9xZNbEoplU+aRFypYkNodC+s+gjOHsmxWqWgMjzQugrzNsSyV7sAK6WKEU0irtb5BeuuZNHzkEsnhz93qo6vlwdvLt1ThMEppVTuNIm4WmCklUh2L4Stc3OsVt7Pi0c6VGPx9hNsijlbdPEppVQuNIkUB63/DJEtYNEYSIrLsdrwDlEE+3nx+pJdRRicUkrlTJNIceDmDn0+sCatWvBMjo+1/L09+HOn6vy27xS/7j1ZxEEqpdT1NIkUF6G1oNNfrXG1ts/LsdrgVpWJCCrD60t26bAoSimX0yRSnLQZBZWawsIxVyavupaPpztP3l6DzbGJLNl+oogDVEqpq2kSKU7cPaDvh5CSBAufy7FavyYRRIf68cbSPWRk6t2IUsp1NIkUN2F1oONY2PE1bP862yoe7m6M6V6LffHnmbchtkjDU0qprDSJFEdtn4SKjWHBs3DhVLZVuterQKPIQN5ZvpeU9IyijU8ppWwerg5AZePyY62Pb7O6/fafcl0VEWFM99oMmbyaZ+ZspkaYP14ebni5u13109/bg861w/Bw178XlFKOp0mkuAqvB7c9DyteseYcqXPndVXa1wihX5MI5m8+xoJc2kZevLMuw9pFOTNapVQppfOJFGcZaTCxMySdgMd+hYDwHKtmZhpSMzKtJf2P5Zk5m4hPSuHH5zrq3YhSqtB0PpGbkbsn3PWR1VtrzgOQnppjVTc3wcfTnbI+noT4e1MpqAxVQ/x49LZoYs9cYrF2B1ZKOYHTkoiI1BKRTVmWcyLylIiUF5FlIrLX/lnOri8i8p6I7BORLSLSNMu+htr194rI0CzlzURkq73NeyIizjoflwmvB30/gJhVsPj5Am9+e51wokL8mPjzAX05USnlcE5LIsaY3caYxsaYxkAz4CLwFTAW+N4YUwP43v4O0BOoYS8jgPEAIlIeeBFoBbQEXryceOw6j2TZroezzsel6t8N7Z6CdVNg3ScF2tTdTRjePorNsYmsOXjaOfEppUqtonqc1QXYb4w5DPQBptrlU4G+9uc+wDRjWQUEiUhFoDuwzBhz2hhzBlgG9LDXlTXGrDLWn9jTsuyr5OnyD4juYr3NfmR1gTa9u2kk5f28mPjLQScFp5QqrYoqidwLzLQ/hxtjjtufTwCXW4sjgJgs28TaZbmVx2ZTfh0RGSEi60RkXUJC9sOJFHtu7tB/sjV0/Jz74dyxfG9axsudIa2rsHxnHPsTzjsxSKVUaeP0JCIiXkBv4Itr19l3EE5/UG+MmWCMaW6MaR4aGurswzlPmXIwaCaknIfZQyAtOd+bPtCmCl4ebkzSuxGllAMVxZ1IT2CDMebyRBlx9qMo7J/xdvlR4JYs20XaZbmVR2ZTXrKF1bF6bB1dDwufzXU2xKxC/L25u2kkX26I5eT5FCcHqZQqLYoiiQzij0dZAPOByz2shgLfZCl/wO6l1RpItB97LQG6iUg5u0G9G7DEXndORFrbvbIeyLKvkq1ub7j1L7BxOqydlO/NHu4QRWp6Jp+tPOzE4JRSpYlTk4iI+AFdgawTZLwKdBWRvcDt9neAhcABYB8wEfgzgDHmNPAvYK29/NMuw64zyd5mP7DImedTrHQcBzV7wuKxcOjXfG0SHerP7XXC+WzVYS6l6nhbSqkbp2+s38ySz8GkLnDxFDzyA5Srmucmaw6eZuDHK3mlb32GtK7i/BiVUiWCvrFeEvmUhUGzIDMdZg6y3mzPQ4uq5WgUGcjkXw/qXCRKqRumSeRmFxwNAz6FhN0wbwRkZuZaXUR45NZqHDx5geU743Ktq5RSedEkUhJEd4Yer8LuhfDDv/Ks3qNeBSLLlWHSLweKIDilVEmmSaSkaPkINBsGv74FW+bkWtXD3Y3h7aNYe+gMG46cKaIAlVIlkSaRkkIEer0OVTvAN6MgNvfOAwOb30JZHw8+/ml/EQWolCqJNImUJO6eMGAqBFSAWfdBYs7vXvp5e/BQ+yiWbI9j/I+aSJRShaNJpKTxC4b7ZkPqBZg1CFIv5lh1dOca9G5Uif8t3sW0lYeKLkalVImhSaQkCqsDd0+G41vg68dzHBrF3U14c2Ajbq8Tzj++2c4X62KyraeUUjnRJFJS1eoBXV+GHV/D9y/nWM3T3Y3372tChxohPP/lFr7bkv/RgZVSKtckIiKds3yOumZdP2cFpRyk7RN2j623YeWHOVbz8XTn4/ub0axKOZ6atYnv9f0RpVQ+5XUn8kaWz19es+4FB8eiHE0E7ngT6twJS8bB1rk5VvX18mDygy2oW6ksj3++gd/2nSzCQJVSN6u8kojk8Dm776o4cnOHfpOgSnv46jHY932OVcv6eDJ1WEuigv14eOo61h3S6XSVUrnLK4mYHD5n910VV54+MGgGhNaG2fdbc5HkoJyfF5893JIKgT4M+2Qtv+/XOxKlVM7ySiLVRGS+iHyb5fPl71F5bKuKE59AGDIX/ELg8wFwcl+OVcMCfPj84VaU8/PivomreXz6eo6cyrmrsFKq9Mp1KHgRuS23jY0xPzk8IicrUUPBF8ap/TC5G3j6wvClULZijlUvpWYw4ecDfPTTfjIyDcPaVWVk5+qU9fEswoCVUsVBTkPBF2g+ERHxBOoDR40x8XnVL45KfRIBOLYRPv0TBFWBYQuhTFCu1U8kJvP6kt18uSGWYD8vnu5ak3tb3IKHu/YQV6q0KNR8IiLykYjUsz8HApuBacBGERnklEiV81VqAvdMh5N7YOa9kHI+1+oVAn14c2Ajvh3Vnugwf174ehu93vuFn/YkUNomNVNKXS2vPyU7GGO225+HAXuMMQ2AZsBfnBqZcq7oTtBvAsSsttpI8jGhVYPIQGaPaM1HQ5qSnJbJ0ClruHv87/ywK06TiVKlVF5JJDXL567A1wDGmBPOCkgVofr9rOFRYlbD9Lut6XbzICL0qF+RZc/cyr/61CPuXAoPfbqOXu/9yndbjulsiUqVMnklkbMi8icRaQK0AxYDiIgHUMbZwakiUL8fDPjE6vY7vR8kJ+ZrM28Pd+5vU5Ufx3TkjQGNSEnPYNSMjXR9+ye+WBdDWkbuMywqpUqGvHpn1QTeAyoA7xhjPrXLuwPdjDHPFkWQjqQN6znY+R188SBUbAhD5uXZ2H6tjEzD4m0n+GDFPnYcP0dEUBmGtatKv6aRlPfzyvd+zlxI5csNsZy7lMbTXWsiou+0KlUcOKR3VkmgSSQXuxfBnAcgrC7c/xX4li/wLowx/Lg7gQ9W7GPd4TN4ugtd64YzsPktdKgRirvb9UnBGMPqg6eZueYIi7aeINW+i/m/QU24s1GlGz4tpdSNK1QSEZH3ctupMeYJB8RWpDSJ5GHPUpg92Hq7/YFvCpVILtt14hxz1sby1cZYzlxMo2KgD/2bRTKg2S1UDvbl1PkUvtwQy6w1MRw4eYEAHw/6NYlgYItb+MvcLZy+kMr3z96Gr5eHA09QKVUYhU0iqcA2YA5wjGvGyzLGTHVwnE6nSSQf9i63ZkYMqQkPfG295X4DUtIz+H5nPLPXxvDz3gSMgXqVyrInLom0DEPzKuUY1LIyvRpUpIyXOwDrDp2m/0crGd25Os92q+WAk1JK3YjCJpFgYABwD5AOzAbmGmPOOilOp9Mkkk/7f4CZgyCgIgyaBWG1HbLbY2cvMW9DLMt3xtOkchCDWlamZnhAtnWfmrWRhdtOsPzp26gc7OuQ4yulCqdQLxsaY04ZYz4yxnTCek8kCNghIvfn86BBIjJXRHaJyE4RaSMiL4nIURHZZC+9stQfJyL7RGS33Xh/ubyHXbZPRMZmKY8SkdV2+WwRyX8LrspddGcY+q01ze6k22HPEofstlJQGUZ1rsHXI9vx4p31ckwgAGN71sHDTXhlwQ6HHFsp5Xj5GrdCRJoCTwJDgEVAzsPAXu1dYLExpjbQCNhpl79tjGlsLwvtY9QF7gXqAT2AD0XEXUTcgQ+AnkBdYJBdF+B/9r6qA2eA4fmMS+XHLS1hxAoIrgYz7oFf38lxql1nqBDow6jO1Vm6I45f9iYU2XGVUvmX17An/xSR9cAzwE9Ac2PMcGNMnn8a2sOk3ApMBjDGpObxGKwPMMsYk2KMOQjsA1rayz5jzAFjTCowC+gjVt/PzsDlmZamAn3ziksVUGAkDFsM9frC8hetOUnSkovs8MPbR1El2JeXv92h754oVQzldSfyAtYjrEbAf4ENIrJFRLaKyJY8to0CEoBPRGSjiEwSET973Sh7P1NEpJxdFgHEZNk+1i7LqTwYOGuMSb+m/DoiMkJE1onIuoQE/Yu2wLx8of8n0OkF2DILPr0Dkopm0AJvD3f+fkdd9sWfZ9rKw0VyTKVU/uWVRKKw/tr/k73caS+XP+fGA2gKjDfGNAEuAGOB8UA00Bg4DrxZyNjzzRgzwRjT3BjTPDQ01NmHK5lE4LYx1sCN8TthQic4uqFIDt2lThi31QzlnWV7OHk+pUiOqZTKn7wa1g9nt2DdGbTPY9+xQKwxZrX9fS7Q1BgTZ4zJMMZkAhOxHlcBHAVuybJ9pF2WU/kpIMgegiVruXKmOnda85C4ecAnPWHzbKcfUkT4x511uZSWweuLdzv9eEqp/MurTaSs3WPqfRHpJpbRwAFgYG7b2oM0xojI5U7+XbB6dmWdBekurPdQAOYD94qIt4hEATWANcBaoIbdE8sLq/F9vrH6Jq8A+tvbDwW+yed5qxtRob7V4B7RHL4aAYvHQUaaUw8ZHerPQ+2jmLM+hs0xZ516LKVU/uX1OOszoBawFXiYP35p9zXG9MnH/kcDn9vtJ42B/wCvZWlT6QQ8DWAPOT8H2IE10ONI+44lHRgFLMHq3TUny/D0zwPPiMg+rDaSyfk6a3Xj/EKsFxFb/xlWfQjT+sJ557Y3je5cnWA/b176djuZOlqwUsVCXi8bbrXnD8HuanscqGyMKbruOQ6mLxs6webZ8O0T4BsM93wGEc2cdqi562N57ovNvDGgEf2bRTrtOEqpqxXqZUPgyjMKY0wGVhvHTZtAlJM0usdqJxF3mNITNk532qH6NYmgaeUgXlmwg4QkbWRXytXySiKNROScvSQBDS9/FpG8ZzBSpUfFRjDiR6jcGr4ZCQuehfTUPDcrKDc34bX+DbmYksFL87fnvYFSyqny6p3lbowpay8BxhiPLJ/LFlWQ6ibhF2zNRdL2CVg7CaZ0g2ObHH6Y6mEBPHl7DRZsPc7ibccdvn+lVP7la9gTpfLN3QO6/QsGToPEozCxEyx6Pl9T7xbEiFurUbdiWV74ejtnLzr+jkcplT+aRJRz1O0Do9ZC84dg9cfwfgvYNs9hY295urvxWv+GnLmYyr++25n3Bkopp9AkopynTBDc8SY88j0EhMPcYTD9bjh9wCG7rx8RyOO3RfPlhlhW7I53yD6VUgWjSUQ5X0QzeGQF9HwNYtbAB63hp9cg9eIN73p0l+pUD/Pnb/O2kpTs3BcelVLX0ySiioabO7R61HrEVfsOWPFveKc+/PQ6XDpT6N16e7jzWv+GHD+XzKuLdjkwYKVUfmgSUUWrbEUY8AkMW2Tdoax4Bd6uD0v+ZjXEF0LTyuV4qF0Un68+wsr9pxwcsFIqN5pElGtUaQuDv4DHfoNavWDVeHi3EXw9EhL2FHh3z3WrReXyvoydt4VLqRlOCFgplR1NIsq1KtSHuyfCExuh+TDYNhc+aAlzH4KU8/neTRkvd169uwGHT13kzaWFH+l3x7FzpKbr5FdK5ZcmEVU8lKsCvV6Hp7dDh2dg+9cwrTdcyP/jqbbRIdzXqjKTfzvIJ78dJLdx4a6Vmp7JP77ZRq/3fuHPn68nXWdRVCpfNImo4sUvBLr8w5r8Km47TOkOZ2Py3s72t1516FI7jJe/3cGoGRvz1WMrPimZwZNWMW3lYTrUCGH5znhe+HpbgZKQUqWVJhFVPNXuZQ2hcj7eSiTx+et55eftwYT7mzO2Z20Wbz9Bn/d/Y9eJnN+W33jkDL3/7ze2Hk3k3Xsb89nwVozsFM2stTG8s3yvo85GqRJLk4gqvqq2g2ELIDMdPukBMWvztZmbm/DYbdHMeLgVSSnp9P3gN+auj72u3uy1R7jn41V4uAtfPt6WPo0jAKuRvn+zSN79fi8zVh9x6CkpVdJoElHFW4UG8NAS8Amy2kj2Ls/3pq2qBbPgifY0viWI577YzNgvt5CclkFqeiZ/+2orz3+5lZZR5fl2VHvqVQq8sp2I8N9+DehYK5QXvt7Ksh1xTjgxpUqGXCelKol0Uqqb1Pl4mN4P4ndC34+g4YB8b5qekcnby/fwwYr91K1YljJe7qw/fIZHb63GmO618HDP/m+pi6npDJqwil0nkpjxSCuaVSnvqLNR6qZT2EmplCoe/MPgwQVQuQ3Me9ga1DGfPNzdGNO9NlMebM7Rs5fYcewc/zeoCeN61ckxgQD4enkw5cEWVAz0YfjUdeyLz3+XY6VKC70TUTeXtGT4cjjs+g46/x1ufa5AmyckpZCakUlEUJl8b3Pk1EX6jf8dbw835v25LeFlfQoatVI3Pb0TUSWDpw8MmAoNBsIP/4LlLxdoePnQAO8CJRCAysG+fDqsBWcvpjJ0yhoSL+lAj0pdpklE3XzcPeCuj6HZg/DrW9akV5nOfTmwfkQgH93fjP0J5xkxbR3JaTq0ilKgSUTdrNzc4E/vQJtRsOZjmD8KMp37i71DjVDeGNCI1QdP8/TsTWRklq5HwUplR5OIunmJQLdXoOM42PS51VaS7typcvs0juCFO+qwaNsJXv52u77Vrko9D1cHoNQNEYGOY8HLD5a+AGmXrDYTT+c1fj/coRoJSSl8/PMBwgK8GdW5htOOpVRxp3ciqmRoOxrueAv2LIEZA+DCSace7vketbmrSQRvLN3D7LX6VrsqvZyaREQkSETmisguEdkpIm1EpLyILBORvfbPcnZdEZH3RGSfiGwRkaZZ9jPUrr9XRIZmKW8mIlvtbd4TEXHm+ahirsVwq8H98Ep4vzlsmlGgnlsF4eYmvNa/IbfWDGXcvK0s17faVSnl7DuRd4HFxpjaQCNgJzAW+N4YUwP43v4O0BOoYS8jgPEAIlIeeBFoBbQEXryceOw6j2TZroeTz0cVd43ugcd+gZCa8PXj8FlfOH3AKYfydHdj/OCmNIgIZOSMDaw/fNopx1GqOHNaEhGRQOBWYDKAMSbVGHMW6ANMtatNBfran/sA04xlFRAkIhWB7sAyY8xpY8wZYBnQw15X1hizylitm9Oy7EuVZmF1YNhi6PUGxK6HD9vCb+9CRrrDD+Xnbb3VXimoDA99ui7XEYOVKomceScSBSQAn4jIRhGZJCJ+QLgx5rhd5wQQbn+OALJOHBFrl+VWHptN+XVEZISIrBORdQkJCTd4Wuqm4OYGLR+BkashujMs+wdM7ATHNjn8UMH+3kx7qCVeHm70/eA3pvx6kEzt/qtKCWcmEQ+gKTDeGNMEuMAfj64AsO8gnP6vzRgzwRjT3BjTPDQ01NmHU8VJYATc+zkMnAbn46xEsuBZOH3QoYe5pbwv341uT9voEP753Q7umbCSQycvOPQYShVHzkwisUCsMWa1/X0uVlKJsx9FYf+Mt9cfBW7Jsn2kXZZbeWQ25UpdTQTq9oGRa6y33Nd/Cu81gVmD4fDvDmt8Dy/rw+ShzXljQCN2nUiix7s/MzkfdyUXU9P5YVccqw/kfypgpYoLpw7AKCK/AA8bY3aLyEuAn73qlDHmVREZC5Q3xvxFRO4ARgG9sBrR3zPGtLQb1tdjJSCADUAzY8xpEVkDPAGsBhYC/2eMWZhbTDoAo+LccVg7EdZNgUtnoGJjaDMS6t0F7p4OOcSJxGT++tVWftgVT4uq5XitfyOiQqz//Y0x7I0/z0+7E/hpTwJrDp4mNSMTdzfhoyHN6Fo3PI+9K1X0chqA0dlJpDEwCfACDgDDsO5+5gCVgcPAQDshCPA+Vg+ri8AwY8w6ez8PAX+1d/tvY8wndnlz4FOgDLAIGG3yOCFNIuqK1IuwZRas/BBO7YWAilY7SvPhUCbohndvjOHLDUd5+dvtpGVkMqJDNeKTUvhpTwLHE5MBqBHmz201Q2lXI4R3lu9l5/FzTB3WkjbRwTd8/PzGuP3YOepULIu7m/aQVzlzSRIpjjSJqOtkZsK+5bDqAzjwI/gEQpvR0Pox8A644d1nvSsJ8PagXfUQbqsVym01Q6mUZUThMxdSGfjxSo4nJjPjkVY0jAy64WPnxhjDfxftYsLPB7ivVWX+3bc++qqVyokmEZsmEZWr45thxX9hzyIoUx7aPQEtR1jDqtwAYwwxpy9RMcgHz1wmwjqRmEz/j37nQko6XzzWhuphN57EcvLhj/t4bfFuaoUHsDsuiadur8FTt9d02vHUzU3nE1EqPyo2gvtmwSM/QERTWP4SvNsIVn5gjctVSCJC5WDfXBMIQIVAH6YPb4W7mxv3T15D7JmLhT5mbmasPsJri3fTu1ElFj7ZgbubRvLO8r3MWK1DuKiC0SSiVHYimsGQL+GhpRBWF5b8Fd5tDKsnOH2k4Kohfnw2vCUXUtK5f/IaTp5Pcej+F2w5zt++3kqnWqG8ObAR7m7Cq3c3oFOtUF74eitLtp9w6PFUyaZJRKncVG4FQ+db87sHR8OiMfBBS9g2z2njcgHUqViWKQ+24HjiJYZOWcO5ZMfMpvjzngSemr2R5lXK8eHgZlfujDzd3fhgcFMaRgbxxMyNrD2kQ7io/NEkolR+VG1vJZLBc8GzDMwdBpO6wKFfnXbI5lXL89GQZuyJS+LhT9dxPuXGhm1Zf/gMj362nuphAUwa2oIyXu5Xrff1soZwiShXhuGfrmX3iaQbOp4qHTSJKJVfIlCjKzz2K/T5EJJOwKd3wIx7IH6nUw7ZsVYYbw1szNrDp2n9n+8ZN28rm2POFngyrN0nknjo07WElfVm6kMtCCyT/fsw5f28mPZQS3w83Rk6ZQ3Hzha+HUiVDto7S6nCSrsEqz+CX96C1PPQeDB0fgECKjj8UBuPnGH6qiMs2HqM5LRMalcI4J4Wt3BXkwiCfL1y3fbgyQvc8/FKRGDuY225pbxvnsfbefwcAz9eSXhZH+Y+1ibPY6iST7v42jSJKIe7cAp+eQPWTARPX+j2L2j6gHXn4mDnktP4dvMxZq+NYUtsIl4ebvSoV4Hb64Zz7lIaJxKTOZ6YTNy5ZI4nXiLuXArnU9IJ8vVkzqNtqBme/y7Dqw6c4oEpa2gUGciMR1rn2bNMlWyaRGyaRJTTnNoP85+Aw79C1Q5w57tWY7yTbD+WyJy1MXy18Sjnkq32Enc3ISzAmwqBPlQo63Pl5+11w4kO9S/wMb7ZdJQnZ21iZKdoxnSv7ehTyFFCUgr/XbSTJ7vUoErwjb2joxxDk4hNk4hyqsxM2DDVGno+Iw06/RVa/xncPZx2yOS0DPbFnyc0wJsQf2+HD1/y/NwtzFkfw/ThrWhXPcSh+86OMYZHpq1j+c54+jeL5I0BjZx+TJU3fdlQqaLg5gbNh9nzmHSCZX+HybfDiW1OO6SPpzv1IwIJL+vjlPGvXuxdl+hQf56avcnh76xk54t1sSzfGU9EUBnmbz7GqSI4pio8TSJKOUPZSnDvDOj/CZyNgQm3wff/gvSb7xeir5cH79/XhMRLaTz3xWanTrgVc/oiL3+7nTbVgvlkWAtS0zOZtTYm7w2Vy2gSUcpZRKB+Pxi1FhoMsBrfJ3SC41tcHVmB1a5Qlr/fUYcfdycw5TfHTuh1WUam4dkvNuMmwhsDG1EzPID21UOYvuowaRmZTjmmunGaRJRyNt/ycNdHcN8cuHjSml3xp9edMue7Mw1pXYXu9cL53+JdbI1NdPj+p/x6kDUHT/Ni73pE2KMbP9i2KscTk1m6Pc7hxytuVh84xcNT19507+ZoElGqqNTsDn9eZc2yuOIVmNwVEva4Oqp8ExH+d3dDQv29GT1zww2/QZ/V7hNJvL5kN93rhXN304gr5Z1qh1G5vC+f/u6cu5/iYtmOOB6YsoblO+N5af52V4dTIJpElCpKvuWh/xSrreTMIfi4gzVCcObN8bgmyNeLd+5twpHTF3nhq60FfnM+O6npmTw9exNly3jwn7saXDWnibub8ECbKqw9dIZtRx1/91McfLEuhsemr6d2xbI8dls0S3fEsWzHzXPnpUlEKVeo38+6K6nW0RoheOqdVlK5CbSMKs+TXWry9aZjfLnh6JXyS6kZrD98mk9/O8gzczbR9a2faPLPpbw0fzsHEs7nuL93v9/DjuPn+G+/hgT7e1+3fkDzWyjj6c7U3w8543RcauLPBxgzdwtto4OZ8XArnu1Wk5rh/rw0fzsXUwt2p+eq1zX0PRGlXMkY2DQDFo8Fk2m97d5smFPednekjEzDfRNXsfVoIj3rV2T7sUT2xCVxueNWaIA3DSMC8fFyZ+n2E6RlGG6rGcqDbatyW81Q3OyuyOsPn2HAR7/Tv1kkr/XP+X2QF77eypx1sawc2znbRHOzMcbw2pLdjP9xP3c0qMhb9zTC28MaEHPtodMM+Gglj95ajXG96uRrf9uPJTLsk7V0rRvOC3fUvW5wTUfQlw1tmkRUsXQ2BuaPsqbnrdYRer8PQbe4OqpcHU+8RO/3f8MYQ4OIQGuJDKJhpPXOymUJSSnMXHOE6asOE5+UQtVgXx5oU5U7Glbkno9Xkp5pWPRkBwJ8sh8UEmBvXBJd3/6ZMd1rMbJTdaed07ajicxcc4SEpBSiw/ypHupPdJg/0aF+ucZXEBmZhr99tZVZa2MY3Koy/+xT/7r3e56fu4W5G2JZ8ER7alcom+v+Yk5fpN/430lNzyTxUho1wvx5b1AT6lTMfbuC0iRi0ySiii1jYP0nsOQFEDfo8R9ocn+xvivJzDSIkK+52VPTM1m8/QRTfz/E+sNnrpzWzEda07pacJ7bD5m0mv0J5/nlL53wcOA4XhdT0/lu83E+X32YzbGJ+Hi6ERFUhsOnLpKe5Z2Y8LLeRIf6UzM8gOHto/I1kOW1ktMyeGrWJhZvP8ETnavzdNea2V67MxdS6fLWT0SF+PHFo22u3LllV+/uj37nZFIKcx9vS/y5FJ6es4nES2n8tWdthratmq//NvmhScSmSUQVe2cOwTej4NAvUP12uPM9CIzIc7ObybajiUxfdZga9i/k/Fi2I45Hpq3jw8FN6dWgYo71jDEs2naCxEtpVCjrQ7g9hlg5X8+rfqHuiUtixuojfLkhlqTkdGqE+TOkdRX6NokgsIwnaRmZxJy+yL748+xPuGD/PM+uE+fw8XTnw8FNaRud/2Fg4s4lM3rmRqsb8511GdYu9/Oeuz6W577YzKv9GnBvy8rXrb+UmsF9k1ax/dg5pg9vRcuo8gCcOp/CmLlb+GFXPJ1rh/F6/+zbmgpKk4hNk4i6KWRmwtpJsPxFcPOEnq9Co0HF+q7E2TIyDR3fWEHFsmWY81ibbOucPJ/CX+xfoNfy8nAjvKw34QE+pGcaNsWcxcvdjV4NKjC4dRWaVymXr7/aD528wMPT1nHw5AVeurMuQ1pXyXO75TviGDN3M8lpmbx6dwP6NM77jwJjDPdOWMWuE0n88OxtVyWC9IxMHpu+nu93xTN+cFN61K943bZTfz/EfxbtIrCMJ28NbESHGqF5HjM3mkRsmkTUTeXUfvhmJBxZCdU6Qa/XIaSGq6NymUm/HOCVBTtZ8ER76lUKvGrdT3sSeHbOZs4lp/G3XnXoWjecE+eSiUtM5sS55Cufjycmk5yWwR0NK9K/2S2U9yv4XClJyWk8NWsT3++KZ1DLyrzcux5eHtc/YktOy+C/C3cydeVh6lYsy3uDmlA9LP+jKe+LT6Lnu79wZ6NKvDWwMWAliL9+tZWZa2L4Z596PNCmao7b7zx+jtEzN7Iv/jyP3lqNZ7vVyjbO/NAkYtMkom46l+9KfngF0i5C21Fw6xjwKn1DpCdeSqP1f77nzkYVr/TmSknP4LXFu5n860FqhQfw7qDGeTZGO0JGpuHNpbv58Mf9tKxanvFDml51t7AnLonRMzayOy6J4e2j+EuPWld6YBXEG0t28/6Kfcx4pBVto0N4d/le3l6+J9/D819KzeBfC3awZNsJFj3VgbAAnzy3yY4mEZsmEXXTOh8Py1+CTZ9D2Ujo/m/r7fdS9ojrb19t5Yv1sawa14XTF1IYPXMTO4+fY2ibKozrVQcfT8d3b83NN5uO8pe5Wwjx92biA82pUzGAz1cf4V/f7SDAx4M3BjSiY62wQu8/OS2Dbm//jIe7MKxtVf7+zXb6N4vk9f4NC9Rofup8yg21jbgkiYjIISAJyADSjTHNReQl4BEgwa72V2PMQrv+OGC4Xf8JY8wSu7wH8C7gDkwyxrxql0cBs4BgYD1wvzEmNbeYNImom96RVbDgOYjbWiofcV3u7tuhRghrD53G18uD1/s3pEudcJfFtCX2LCOmrSfxUhpNqwTx275T3FozlDcHNCI04MYbtX/cHc+Dn6wFoGOtUCY+0LzIZ5p0ZRJpbow5maXsJeC8MeaNa+rWBWYCLYFKwHKgpr16D9AViAXWAoOMMTtEZA4wzxgzS0Q+AjYbY8bnFpMmEVUiZKTDuil/POJqMxI6PAs+zn+MUxwMnrSK3/adokONEN4c0IiwsoV7RONI8eeSeXT6erYdTeT5HrV5qF1Ujl1zC2PcvK0cPnWBiQ80x8/beZOc5eRmSCLjAIwx/7W/LwFesle/ZIzpnrUe8CrW3UwFY0y6iLTJWi8nmkRUiXI+Hpa9CJtngG8IdBwLzR4Ed8e8GFdcHTl1kU2xZ/lTg4oO/UV9o9IzMjl7KY2QEvBW/bVcNbOhAZaKyHoRGZGlfJSIbBGRKSJSzi6LALLOPhNrl+VUHgycNcakX1N+HREZISLrRGRdQkJCdlWUujn5h8Fd4+GRFRBWBxY+Bx+2hp3fWi8vllCVg33p3ahSsUogAB7ubiUygeTG2UmkvTGmKdATGCkitwLjgWigMXAceNPJMWCMmWCMaW6MaR4aemN9pZUqliKawtBvYdBsEHeYPQSm9ICYta6OTJVwTk0ixpij9s944CugpTEmzhiTYYzJBCZitYEAHAWyDhYUaZflVH4KCBIRj2vKlSqdRKBWD3j8d/jTO3D6gDW/+5wH4OQ+V0enSiinJRER8RORgMufgW7ANhHJ+mrlXcA2+/N84F4R8bZ7XdUA1mA1pNcQkSgR8QLuBeYbqzFnBdDf3n4o8I2zzkepm4a7BzQfBk9shNvGwt5l8H5zmDVY70yUwzmziT8c+Mrux+wBzDDGLBaRz0SkMVZ7ySHgUQBjzHa7t9UOIB0YaYzJABCRUcASrC6+U4wxl6f+eh6YJSKvABuByU48H6VuLt7+0GkctBgOqz+2Xljc9R1UbgPtnoQa3cFNpxRSN0ZfNlSqtEg5Dxs/s2ZSTIyBkJrQ9gloOBA8SldjsCo4V/XOUkoVF97+0Ppx6zFXv0lW4pg/Ct5pAMv+AXE7XB2hugnpnYhSpZUxcGCF9ahr7zIwGRDeABrdA/X7Q9mch1tXpY+OnWXTJKJUNi6chG3zYMssOLremhQr6jZoeA/UudO6i1GlmiYRmyYRpfJwch9smW0tZw+DRxmo3QsaDIDoLuBR8KHT1c1Pk4hNk4hS+WQMxKyGrV9YdymXToNPENTrayWUym21d1cpoknEpklEqULISIP9K6yEsmsBpF2AshFQ/26o1Qsim5f48bpKO00iNk0iSt2g1Auwe5GVUPYth8x08PKHqu2toemrdYTQWqVunpOSLqckUvTjCSulbm5eftCgv7VcOgMHf7F6eR34EfYstuoEVLSSSdRtENEMgqvro68SSpOIUqrwypSDur2tBeDMYSuZHFgBe5bA5plWuZc/VGgIlRpDxcbWz+Dq4Fa0sxAqx9MkopRynHJVoNlQa8nMhISdcGwTHN9k/Vz3CaRfsup6+kF4PevRV1gdCK1tLWUr6aOwm4i2iSilik5GOpzcA8c2WoklboeVaC6e+qOOd6CVWEJrQbmqEFQFgm6BoMrgX0Efi7mItokopVzP3QPC61pLk8F/lF84CfE7IWGXvey22lcuXDOJnLuX1SssqDKUj4Lw+lChgXVH4x1QtOeiAE0iSqniwC8EojpYS1apFyAxFs7GWC8+JsbA2SPW9x3fwPpP/6hbLgoq1LeGbqlQ30o0fqHgG6zdj51Ik4hSqvjy8vvj0da1jIFzx+DEVojbCie2WZ93foc100QWPkFWQvELtRKWb7A1lItXgP3T/+rvHj7g5mEt7p5WB4DL3908rPVeftoxAE0iSqmblQgERlhLrR5/lKectx6JnTtqPQ67cMr+mWC1vZzcYz0+Sz0P6ck3FoO7N3j5gqe9ePnaCSkgm6WstfiHWe08gZHg6XNjxy8GNIkopUoWb3/rDXquawO+XkY6pCZZiSf1vP0zCdJTrJcoM9MhMyPL53Tr7f20S/ZyAVIvQtpF69Fb2iVrP+eOQUrSH0tGSvbH9w+3HrsF2h0HAiOtROPla93pePpZPy8nJ3dP6w7MZGb5aS8YK6l5+ljjnbkXza93TSJKqdLL3cN616VMOeceJz3FSibJiZB0IkvbzmGrfefYRtj5LWSmOe6Ybp72HZIPeJaxEsuIFdZnB9IkopRSzubhbS1+IRAcnX2dzIw/HrOlXrDvbs5bdzqpF6y7nvRUqx1G7G7O4mYvAghkpP5xl5R+Kcsdk/3d3fEjMGsSUUqp4sDNHQLCgXBXR1Ig+taOUkqpQtMkopRSqtA0iSillCo0TSJKKaUKTZOIUkqpQtMkopRSqtA0iSillCo0TSJKKaUKrdRNSiUiCcDhQm4eApx0YDiOonEVjMZVMBpXwZTUuKoYY0KvLSx1SeRGiMi67Gb2cjWNq2A0roLRuAqmtMWlj7OUUkoVmiYRpZRShaZJpGAmuDqAHGhcBaNxFYzGVTClKi5tE1FKKVVoeieilFKq0DSJKKWUKjRNIvkkIodEZKuIbBKRdS6MY4qIxIvItixl5UVkmYjstX86ea7PfMf1kogcta/ZJhHpVcQx3SIiK0Rkh4hsF5En7XKXXq9c4nLp9bJj8BGRNSKy2Y7tZbs8SkRWi8g+EZktIo6fIq9wcX0qIgezXLPGRRmXHYO7iGwUke/s7y69VrnE5ZRrpUmkYDoZYxq7uA/4p0CPa8rGAt8bY2oA39vfi9qnXB8XwNv2NWtsjFlYxDGlA88aY+oCrYGRIlIX11+vnOIC114vgBSgszGmEdAY6CEirYH/2bFVB84Aw4tJXABjslyzTUUcF8CTwM4s3119rS67Ni5wwrXSJHKTMcb8DJy+prgPMNX+PBXoW5QxQY5xuZQx5rgxZoP9OQnrH1QELr5eucTlcsZy3v7qaS8G6AzMtctdcc1yisulRCQSuAOYZH8XXHytsovLmTSJ5J8BlorIehEZ4epgrhFujDlufz5B8ZqkeZSIbLEfdxX5Y7bLRKQq0ARYTTG6XtfEBcXgetmPQTYB8cAyYD9w1hiTbleJxQVJ79q4jDGXr9m/7Wv2toh4F3FY7wB/ATLt78EUg2uVTVyXOfxaaRLJv/bGmKZAT6zHD7e6OqDsGKvPtsv/QrONB6KxHj8cB950RRAi4g98CTxljDmXdZ0rr1c2cRWL62WMyTDGNAYigZZAbVfEca1r4xKR+sA4rPhaAOWB54sqHhH5ExBvjFlfVMfMj1zicsq10iSST8aYo/bPeOArrH9cxUWciFQEsH/GuzgeAIwxcfY//ExgIi64ZiLiifWL+nNjzDy72OXXK7u4isP1ysoYcxZYAbQBgkTEw14VCRwtBnH1sB8NGmNMCvAJRXvN2gG9ReQQMAvrMda7uP5aXReXiEx31rXSJJIPIuInIgGXPwPdgG25b1Wk5gND7c9DgW9cGMsVl39R2+6iiK+Z/Xx6MrDTGPNWllUuvV45xeXq62XHECoiQfbnMkBXrDabFUB/u5orrll2ce3K8seAYLU9FNk1M8aMM8ZEGmOqAvcCPxhjBuPia5VDXEOcda088q6isJ6Zf2VdezyAGcaYxa4IRERmAh2BEBGJBV4EXgXmiMhwrGHuBxaTuDra3QgNcAh4tIjDagfcD2y1n6UD/BXXX6+c4hrk4usFUBGYKiLuWH9kzjHGfCciO4BZIvIKsBErCRaHuH4QkVBAgE3AY0UcV3aex7XXKiefO+Na6bAnSimlCk0fZymllCo0TSJKKaUKTZOIUkqpQtMkopRSqtA0iSillCo0TSJKFXMi0vHySKxKFTeaRJRSShWaJhGlHEREhthzXmwSkY/tAQPP24PdbReR7+2XvRCRxiKyyh4M76vLgy2KSHURWS7WvBkbRCTa3r2/iMwVkV0i8rn91jEi8qpY85JsEZE3XHTqqhTTJKKUA4hIHeAeoJ09SGAGMBjwA9YZY+oBP2G9yQ8wDXjeGNMQ2Jql/HPgA3vejLZYAzGCNdLvU0BdoBrQTkSCsYZHqWfv5xVnnqNS2dEkopRjdAGaAWvtoUy6YP2yzwRm23WmA+1FJBAIMsb8ZJdPBW61x2eLMMZ8BWCMSTbGXLTrrDHGxNqDM24CqgKJQDIwWUT6AZfrKlVkNIko5RgCTM0ya1wtY8xL2dQr7DhDKVk+ZwAe9pwVLbEmQPoT4JLx3FTppklEKcf4HugvImFwZR73Klj/xi6P6Hof8KsxJhE4IyId7PL7gZ/sWQ5jRaSvvQ9vEfHN6YD2fCSB9jS6TwONnHBeSuVKR/FVygGMMTtE5AWs2S/dgDRgJHABawKlF7DmLbnH3mQo8JGdJA4Aw+zy+4GPReSf9j4G5HLYAOAbEfHBuhN6xsGnpVSedBRfpZxIRM4bY/xdHYdSzqKPs5RSShWa3okopZQqNL0TUUopVWiaRJRSShWaJhGllFKFpklEKaVUoWkSUUopVWj/D/YYfrSWAMmSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting RMSE of of train and test data across epochs ignoring first 5 epochs\n",
    "plt.figure(1)\n",
    "plt.plot(range(5,len(history.history['val_root_mean_squared_error'])), history.history['val_root_mean_squared_error'][5:])\n",
    "plt.plot(range(5,len(history.history['root_mean_squared_error'])), history.history['root_mean_squared_error'][5:])\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['test','train'])\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae1d64",
   "metadata": {},
   "source": [
    "## e) Using the model from the best epoch, report the test R 2 value and show the top 30 test samples with the largest errors. List down any trends you find in these samples and suggest ways to reduce these errors. (Tip: Add the prediction error as a column in the DataFrame and sort by it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a10f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# loading best model\n",
    "best_model = tf.keras.models.load_model('./data/models/PartB_Q2/2d/best_model42',custom_objects ={'r2':r2})\n",
    "test_prediction = best_model.predict(val_ds)\n",
    "\n",
    "#append the predictions to the val dataframe\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "val_dataframe[\"test_prediction\"] = test_prediction\n",
    "\n",
    "#get the absolute error of test_prediction and the actual resale_price\n",
    "val_dataframe[\"error\"] = abs(val_dataframe[\"test_prediction\"] - val_dataframe[\"resale_price\"])\n",
    "\n",
    "#sort and show top 30 largest error\n",
    "val_dataframe.sort_values(by=['error'], inplace=True, ascending=False)\n",
    "\n",
    "val_to_save = val_dataframe.head(30)\n",
    "#save dataframe for largest errors to csv\n",
    "val_to_save.to_csv(\"./data/top_30_errors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ccce2",
   "metadata": {},
   "source": [
    "### Top 30 largest errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49a531ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "      <th>test_prediction</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68105</th>\n",
       "      <td>2</td>\n",
       "      <td>0.353</td>\n",
       "      <td>2.413</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>37</td>\n",
       "      <td>89.917</td>\n",
       "      <td>107.000</td>\n",
       "      <td>13</td>\n",
       "      <td>1232000.000</td>\n",
       "      <td>261010.391</td>\n",
       "      <td>970989.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83720</th>\n",
       "      <td>10</td>\n",
       "      <td>0.336</td>\n",
       "      <td>2.536</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>29</td>\n",
       "      <td>89.167</td>\n",
       "      <td>119.000</td>\n",
       "      <td>12</td>\n",
       "      <td>1200000.000</td>\n",
       "      <td>260651.938</td>\n",
       "      <td>939348.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78257</th>\n",
       "      <td>8</td>\n",
       "      <td>0.353</td>\n",
       "      <td>2.413</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>37</td>\n",
       "      <td>89.333</td>\n",
       "      <td>107.000</td>\n",
       "      <td>14</td>\n",
       "      <td>1258000.000</td>\n",
       "      <td>384020.344</td>\n",
       "      <td>873979.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67656</th>\n",
       "      <td>2</td>\n",
       "      <td>1.277</td>\n",
       "      <td>8.340</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>8</td>\n",
       "      <td>64.833</td>\n",
       "      <td>60.000</td>\n",
       "      <td>3</td>\n",
       "      <td>254000.000</td>\n",
       "      <td>1122437.500</td>\n",
       "      <td>868437.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83126</th>\n",
       "      <td>10</td>\n",
       "      <td>0.321</td>\n",
       "      <td>2.419</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>37</td>\n",
       "      <td>89.250</td>\n",
       "      <td>105.000</td>\n",
       "      <td>10</td>\n",
       "      <td>1130000.000</td>\n",
       "      <td>290942.969</td>\n",
       "      <td>839057.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70637</th>\n",
       "      <td>5</td>\n",
       "      <td>0.767</td>\n",
       "      <td>6.328</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>29</td>\n",
       "      <td>90.333</td>\n",
       "      <td>120.000</td>\n",
       "      <td>6</td>\n",
       "      <td>1070000.000</td>\n",
       "      <td>241079.141</td>\n",
       "      <td>828920.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75090</th>\n",
       "      <td>6</td>\n",
       "      <td>0.971</td>\n",
       "      <td>15.151</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9</td>\n",
       "      <td>61.917</td>\n",
       "      <td>74.000</td>\n",
       "      <td>0</td>\n",
       "      <td>250000.000</td>\n",
       "      <td>1069031.500</td>\n",
       "      <td>819031.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83124</th>\n",
       "      <td>10</td>\n",
       "      <td>0.321</td>\n",
       "      <td>2.419</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>37</td>\n",
       "      <td>89.250</td>\n",
       "      <td>105.000</td>\n",
       "      <td>16</td>\n",
       "      <td>1248000.000</td>\n",
       "      <td>444071.844</td>\n",
       "      <td>803928.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84648</th>\n",
       "      <td>10</td>\n",
       "      <td>0.515</td>\n",
       "      <td>4.108</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.024</td>\n",
       "      <td>29</td>\n",
       "      <td>90.500</td>\n",
       "      <td>114.000</td>\n",
       "      <td>12</td>\n",
       "      <td>1090000.000</td>\n",
       "      <td>289993.219</td>\n",
       "      <td>800006.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66509</th>\n",
       "      <td>1</td>\n",
       "      <td>1.723</td>\n",
       "      <td>9.579</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9</td>\n",
       "      <td>94.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>5</td>\n",
       "      <td>320000.000</td>\n",
       "      <td>1117669.500</td>\n",
       "      <td>797669.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71144</th>\n",
       "      <td>5</td>\n",
       "      <td>0.443</td>\n",
       "      <td>6.855</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>8</td>\n",
       "      <td>56.083</td>\n",
       "      <td>59.000</td>\n",
       "      <td>0</td>\n",
       "      <td>226000.000</td>\n",
       "      <td>1019715.688</td>\n",
       "      <td>793715.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72433</th>\n",
       "      <td>5</td>\n",
       "      <td>2.029</td>\n",
       "      <td>17.167</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "      <td>94.250</td>\n",
       "      <td>47.000</td>\n",
       "      <td>2</td>\n",
       "      <td>230000.000</td>\n",
       "      <td>1021938.188</td>\n",
       "      <td>791938.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73057</th>\n",
       "      <td>6</td>\n",
       "      <td>0.828</td>\n",
       "      <td>6.370</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>29</td>\n",
       "      <td>90.250</td>\n",
       "      <td>120.000</td>\n",
       "      <td>9</td>\n",
       "      <td>1060000.000</td>\n",
       "      <td>274760.500</td>\n",
       "      <td>785239.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80555</th>\n",
       "      <td>9</td>\n",
       "      <td>0.192</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.048</td>\n",
       "      <td>30</td>\n",
       "      <td>94.250</td>\n",
       "      <td>112.000</td>\n",
       "      <td>7</td>\n",
       "      <td>1153000.000</td>\n",
       "      <td>369164.812</td>\n",
       "      <td>783835.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73363</th>\n",
       "      <td>6</td>\n",
       "      <td>0.438</td>\n",
       "      <td>2.507</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>26</td>\n",
       "      <td>89.500</td>\n",
       "      <td>94.000</td>\n",
       "      <td>16</td>\n",
       "      <td>1085000.000</td>\n",
       "      <td>305826.812</td>\n",
       "      <td>779173.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75792</th>\n",
       "      <td>7</td>\n",
       "      <td>0.321</td>\n",
       "      <td>2.419</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>37</td>\n",
       "      <td>89.417</td>\n",
       "      <td>107.000</td>\n",
       "      <td>10</td>\n",
       "      <td>1185000.000</td>\n",
       "      <td>408128.344</td>\n",
       "      <td>776871.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71911</th>\n",
       "      <td>5</td>\n",
       "      <td>0.197</td>\n",
       "      <td>5.422</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>30</td>\n",
       "      <td>93.917</td>\n",
       "      <td>117.000</td>\n",
       "      <td>8</td>\n",
       "      <td>1150000.000</td>\n",
       "      <td>373873.469</td>\n",
       "      <td>776126.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71398</th>\n",
       "      <td>5</td>\n",
       "      <td>0.938</td>\n",
       "      <td>15.713</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8</td>\n",
       "      <td>56.333</td>\n",
       "      <td>65.000</td>\n",
       "      <td>4</td>\n",
       "      <td>243000.000</td>\n",
       "      <td>1018227.312</td>\n",
       "      <td>775227.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86090</th>\n",
       "      <td>11</td>\n",
       "      <td>0.365</td>\n",
       "      <td>2.638</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.053</td>\n",
       "      <td>29</td>\n",
       "      <td>89.167</td>\n",
       "      <td>109.000</td>\n",
       "      <td>7</td>\n",
       "      <td>985000.000</td>\n",
       "      <td>212980.234</td>\n",
       "      <td>772019.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64927</th>\n",
       "      <td>0</td>\n",
       "      <td>0.992</td>\n",
       "      <td>17.554</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30</td>\n",
       "      <td>73.000</td>\n",
       "      <td>126.000</td>\n",
       "      <td>0</td>\n",
       "      <td>333000.000</td>\n",
       "      <td>1103747.750</td>\n",
       "      <td>770747.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80722</th>\n",
       "      <td>9</td>\n",
       "      <td>0.567</td>\n",
       "      <td>2.662</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.121</td>\n",
       "      <td>37</td>\n",
       "      <td>89.333</td>\n",
       "      <td>105.000</td>\n",
       "      <td>8</td>\n",
       "      <td>1080000.000</td>\n",
       "      <td>310114.969</td>\n",
       "      <td>769885.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73546</th>\n",
       "      <td>6</td>\n",
       "      <td>0.068</td>\n",
       "      <td>9.176</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>91.000</td>\n",
       "      <td>116.000</td>\n",
       "      <td>12</td>\n",
       "      <td>1040000.000</td>\n",
       "      <td>273552.375</td>\n",
       "      <td>766447.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64152</th>\n",
       "      <td>0</td>\n",
       "      <td>0.522</td>\n",
       "      <td>11.193</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8</td>\n",
       "      <td>65.250</td>\n",
       "      <td>60.000</td>\n",
       "      <td>3</td>\n",
       "      <td>250000.000</td>\n",
       "      <td>1013362.250</td>\n",
       "      <td>763362.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68109</th>\n",
       "      <td>2</td>\n",
       "      <td>0.882</td>\n",
       "      <td>13.951</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9</td>\n",
       "      <td>63.583</td>\n",
       "      <td>74.000</td>\n",
       "      <td>1</td>\n",
       "      <td>260000.000</td>\n",
       "      <td>1012222.812</td>\n",
       "      <td>752222.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67841</th>\n",
       "      <td>2</td>\n",
       "      <td>0.776</td>\n",
       "      <td>6.297</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>29</td>\n",
       "      <td>90.500</td>\n",
       "      <td>120.000</td>\n",
       "      <td>6</td>\n",
       "      <td>1082000.000</td>\n",
       "      <td>333108.719</td>\n",
       "      <td>748891.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75217</th>\n",
       "      <td>6</td>\n",
       "      <td>0.824</td>\n",
       "      <td>15.271</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30</td>\n",
       "      <td>66.500</td>\n",
       "      <td>121.000</td>\n",
       "      <td>1</td>\n",
       "      <td>380000.000</td>\n",
       "      <td>1125974.250</td>\n",
       "      <td>745974.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75513</th>\n",
       "      <td>7</td>\n",
       "      <td>1.359</td>\n",
       "      <td>6.725</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.014</td>\n",
       "      <td>40</td>\n",
       "      <td>68.583</td>\n",
       "      <td>186.000</td>\n",
       "      <td>3</td>\n",
       "      <td>958000.000</td>\n",
       "      <td>213686.500</td>\n",
       "      <td>744313.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73351</th>\n",
       "      <td>6</td>\n",
       "      <td>0.474</td>\n",
       "      <td>8.936</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>68.167</td>\n",
       "      <td>146.000</td>\n",
       "      <td>3</td>\n",
       "      <td>990000.000</td>\n",
       "      <td>248301.109</td>\n",
       "      <td>741698.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64383</th>\n",
       "      <td>0</td>\n",
       "      <td>0.298</td>\n",
       "      <td>2.290</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.048</td>\n",
       "      <td>18</td>\n",
       "      <td>95.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>10</td>\n",
       "      <td>968000.000</td>\n",
       "      <td>226651.406</td>\n",
       "      <td>741348.594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85901</th>\n",
       "      <td>11</td>\n",
       "      <td>1.627</td>\n",
       "      <td>10.982</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>36</td>\n",
       "      <td>57.417</td>\n",
       "      <td>104.000</td>\n",
       "      <td>4</td>\n",
       "      <td>376000.000</td>\n",
       "      <td>1116069.750</td>\n",
       "      <td>740069.750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       month  dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "68105      2                0.353          2.413              0.034   \n",
       "83720     10                0.336          2.536              0.017   \n",
       "78257      8                0.353          2.413              0.034   \n",
       "67656      2                1.277          8.340              0.017   \n",
       "83126     10                0.321          2.419              0.034   \n",
       "70637      5                0.767          6.328              0.034   \n",
       "75090      6                0.971         15.151              0.017   \n",
       "83124     10                0.321          2.419              0.034   \n",
       "84648     10                0.515          4.108              0.017   \n",
       "66509      1                1.723          9.579              0.017   \n",
       "71144      5                0.443          6.855              0.017   \n",
       "72433      5                2.029         17.167              0.017   \n",
       "73057      6                0.828          6.370              0.034   \n",
       "80555      9                0.192          2.345              0.017   \n",
       "73363      6                0.438          2.507              0.034   \n",
       "75792      7                0.321          2.419              0.034   \n",
       "71911      5                0.197          5.422              0.017   \n",
       "71398      5                0.938         15.713              0.017   \n",
       "86090     11                0.365          2.638              0.017   \n",
       "64927      0                0.992         17.554              0.017   \n",
       "80722      9                0.567          2.662              0.034   \n",
       "73546      6                0.068          9.176              0.017   \n",
       "64152      0                0.522         11.193              0.025   \n",
       "68109      2                0.882         13.951              0.017   \n",
       "67841      2                0.776          6.297              0.034   \n",
       "75217      6                0.824         15.271              0.017   \n",
       "75513      7                1.359          6.725              0.017   \n",
       "73351      6                0.474          8.936              0.017   \n",
       "64383      0                0.298          2.290              0.017   \n",
       "85901     11                1.627         10.982              0.025   \n",
       "\n",
       "       eigenvector_centrality  flat_model_type  remaining_lease_years  \\\n",
       "68105                   0.121               37                 89.917   \n",
       "83720                   0.004               29                 89.167   \n",
       "78257                   0.121               37                 89.333   \n",
       "67656                   0.002                8                 64.833   \n",
       "83126                   0.121               37                 89.250   \n",
       "70637                   0.016               29                 90.333   \n",
       "75090                   0.000                9                 61.917   \n",
       "83124                   0.121               37                 89.250   \n",
       "84648                   0.024               29                 90.500   \n",
       "66509                   0.001                9                 94.000   \n",
       "71144                   0.007                8                 56.083   \n",
       "72433                   0.000                4                 94.250   \n",
       "73057                   0.016               29                 90.250   \n",
       "80555                   0.048               30                 94.250   \n",
       "73363                   0.121               26                 89.500   \n",
       "75792                   0.121               37                 89.417   \n",
       "71911                   0.005               30                 93.917   \n",
       "71398                   0.000                8                 56.333   \n",
       "86090                   0.053               29                 89.167   \n",
       "64927                   0.000               30                 73.000   \n",
       "80722                   0.121               37                 89.333   \n",
       "73546                   0.001               30                 91.000   \n",
       "64152                   0.001                8                 65.250   \n",
       "68109                   0.000                9                 63.583   \n",
       "67841                   0.016               29                 90.500   \n",
       "75217                   0.000               30                 66.500   \n",
       "75513                   0.014               40                 68.583   \n",
       "73351                   0.001               40                 68.167   \n",
       "64383                   0.048               18                 95.000   \n",
       "85901                   0.001               36                 57.417   \n",
       "\n",
       "       floor_area_sqm  storey_range  resale_price  test_prediction      error  \n",
       "68105         107.000            13   1232000.000       261010.391 970989.609  \n",
       "83720         119.000            12   1200000.000       260651.938 939348.062  \n",
       "78257         107.000            14   1258000.000       384020.344 873979.656  \n",
       "67656          60.000             3    254000.000      1122437.500 868437.500  \n",
       "83126         105.000            10   1130000.000       290942.969 839057.031  \n",
       "70637         120.000             6   1070000.000       241079.141 828920.859  \n",
       "75090          74.000             0    250000.000      1069031.500 819031.500  \n",
       "83124         105.000            16   1248000.000       444071.844 803928.156  \n",
       "84648         114.000            12   1090000.000       289993.219 800006.781  \n",
       "66509          68.000             5    320000.000      1117669.500 797669.500  \n",
       "71144          59.000             0    226000.000      1019715.688 793715.688  \n",
       "72433          47.000             2    230000.000      1021938.188 791938.188  \n",
       "73057         120.000             9   1060000.000       274760.500 785239.500  \n",
       "80555         112.000             7   1153000.000       369164.812 783835.188  \n",
       "73363          94.000            16   1085000.000       305826.812 779173.188  \n",
       "75792         107.000            10   1185000.000       408128.344 776871.656  \n",
       "71911         117.000             8   1150000.000       373873.469 776126.531  \n",
       "71398          65.000             4    243000.000      1018227.312 775227.312  \n",
       "86090         109.000             7    985000.000       212980.234 772019.766  \n",
       "64927         126.000             0    333000.000      1103747.750 770747.750  \n",
       "80722         105.000             8   1080000.000       310114.969 769885.031  \n",
       "73546         116.000            12   1040000.000       273552.375 766447.625  \n",
       "64152          60.000             3    250000.000      1013362.250 763362.250  \n",
       "68109          74.000             1    260000.000      1012222.812 752222.812  \n",
       "67841         120.000             6   1082000.000       333108.719 748891.281  \n",
       "75217         121.000             1    380000.000      1125974.250 745974.250  \n",
       "75513         186.000             3    958000.000       213686.500 744313.500  \n",
       "73351         146.000             3    990000.000       248301.109 741698.891  \n",
       "64383          92.000            10    968000.000       226651.406 741348.594  \n",
       "85901         104.000             4    376000.000      1116069.750 740069.750  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd7bc6",
   "metadata": {},
   "source": [
    "Best epoch is 42. we can get the respective val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "260cd337",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for the model from the best epoch: 0.8673686385154724\n"
     ]
    }
   ],
   "source": [
    "print( 'R2 for the model from the best epoch: {}'.format(history.history['val_r2'][41]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10ecc0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37    6\n",
       "29    6\n",
       "30    5\n",
       "8     4\n",
       "9     3\n",
       "40    2\n",
       "4     1\n",
       "26    1\n",
       "18    1\n",
       "36    1\n",
       "Name: flat_model_type, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataframe.head(30).flat_model_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0c74e",
   "metadata": {},
   "source": [
    "it seems like most of flat model types are type 29 and 30."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralNetworks",
   "language": "python",
   "name": "neuralnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
